{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/chroma.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "import os\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms.fake import FakeListLLM\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Determine the name of the environment variable you want to use for the OpenAPI key\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"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\"\n",
    "print(os.environ['OPENAI_API_KEY'])\n",
    "loader = TextLoader('article.txt' )\n",
    "index_creator = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=Chroma, \n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    ").from_loaders([loader])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Config',\n",
       " '_abc_impl',\n",
       " '_calculate_keys',\n",
       " '_copy_and_set_values',\n",
       " '_decompose_class',\n",
       " '_enforce_dict_if_root',\n",
       " '_get_value',\n",
       " '_init_private_attributes',\n",
       " '_iter',\n",
       " 'construct',\n",
       " 'copy',\n",
       " 'dict',\n",
       " 'from_orm',\n",
       " 'json',\n",
       " 'parse_file',\n",
       " 'parse_obj',\n",
       " 'parse_raw',\n",
       " 'query',\n",
       " 'query_with_sources',\n",
       " 'schema',\n",
       " 'schema_json',\n",
       " 'update_forward_refs',\n",
       " 'validate',\n",
       " 'vectorstore']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_creator.__class__.mro()\n",
    "[attr for attr in dir(index_creator) if attr.startswith('__') is False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tanguyrenaudie/miniforge3/envs/language/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "prompt_template = \"\"\"Use the context below to write a 100 word blog post about the topic below:\n",
    "    Context: {context}\n",
    "    Topic: {topic}\n",
    "    Blog post:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"topic\"]\n",
    ")\n",
    "\n",
    "llm = FakeListLLM(responses = ['France', 'Germany', 'Italy', 'Spain', 'United Kingdom', 'USA', 'China', 'India', 'Japan', 'Russia'])\n",
    "llm = HuggingFaceHub()\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 2009, donor countries promised to mobilize $100\n",
      "The World Bank and the donor countries that contro\n",
      "Getting new money in the door is important, but it\n",
      "For years, climate financing took a back seat to t\n",
      "[{'text': ' What'}, {'text': ' In'}, {'text': ' Give'}, {'text': ' In'}]\n"
     ]
    }
   ],
   "source": [
    "def getattributes(obj):\n",
    "    return [attr for attr in dir(obj) if attr.startswith('__') is False]\n",
    "\n",
    "def generate_blog_post(topic):\n",
    "    docs = index_creator.vectorstore.similarity_search(topic, k=4)\n",
    "    for k in range(4):\n",
    "        print(docs[k].page_content[:50])\n",
    "    inputs = [{\"context\": doc.page_content, \"topic\": topic} for doc in docs]\n",
    "    return chain.apply(inputs)\n",
    "\n",
    "generate_blog_post('donor countries promised')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chroma VectorStore Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "def getDocs():\n",
    "    for file in os.listdir():\n",
    "        if file.endswith(\".txt\"):\n",
    "            with open(file, \"r\") as f:\n",
    "                github_url = f\"{file}\"\n",
    "                yield Document(page_content=f.read(), metadata={\"source\": github_url})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getDocs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sources \u001b[39m=\u001b[39m getDocs()\n\u001b[1;32m      3\u001b[0m source_chunks \u001b[39m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m splitter \u001b[39m=\u001b[39m CharacterTextSplitter(separator\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m, chunk_size\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m, chunk_overlap\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'getDocs' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "sources = getDocs()\n",
    "\n",
    "source_chunks = []\n",
    "splitter = CharacterTextSplitter(separator=\" \", chunk_size=512, chunk_overlap=0)\n",
    "for source in sources:\n",
    "    print(source.metadata)\n",
    "    print(source.page_content[:50])\n",
    "    for chunk in splitter.split_text(source.page_content):\n",
    "        print(len(chunk))\n",
    "        source_chunks.append(Document(page_content=chunk, metadata=source.metadata))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'source_chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m source_chunks\n",
      "\u001b[0;31mNameError\u001b[0m: name 'source_chunks' is not defined"
     ]
    }
   ],
   "source": [
    "source_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'source_chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m search_index \u001b[39m=\u001b[39m Chroma\u001b[39m.\u001b[39mfrom_documents(source_chunks, OpenAIEmbeddings(), persist_directory \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdbdir\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m search_index\u001b[39m.\u001b[39mpersist()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'source_chunks' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "search_index = Chroma.from_documents(source_chunks, OpenAIEmbeddings(), persist_directory = 'dbdir')\n",
    "search_index.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getattributes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m getattributes(search_index)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'getattributes' is not defined"
     ]
    }
   ],
   "source": [
    "getattributes(search_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding new docs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3be928ca-d47e-11ed-8349-22414b0296f5']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_index.add_texts([\"Ankush went to Princeton\"])\n",
    "search_index.add_documents([Document(page_content=\"Ankush went to Princeton\", metadata={'source': 'sentence1'})])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Similarity Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_LANGCHAIN_DEFAULT_COLLECTION_NAME', '_abc_impl', '_client', '_client_settings', '_collection', '_embedding_function', '_persist_directory', 'add_documents', 'add_texts', 'as_retriever', 'delete_collection', 'from_documents', 'from_texts', 'max_marginal_relevance_search', 'max_marginal_relevance_search_by_vector', 'persist', 'similarity_search', 'similarity_search_by_vector', 'similarity_search_with_score']\n"
     ]
    }
   ],
   "source": [
    "print(getattributes(search_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'love'\n",
    "docs = search_index.similarity_search(topic, k=4)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New index from saved pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceHub()\n",
    "chain = LLMChain(llm=llm, prompt=PROMPT)\n",
    "def generatefromsaved():\n",
    "    docs = index_creator.vectorstore.similarity_search(topic, k=4)\n",
    "    inputs = [{\"context\": doc.page_content, \"topic\": topic} for doc in docs]\n",
    "    print(chain.apply(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: dbdir\n"
     ]
    }
   ],
   "source": [
    "vectorstore2 =  Chroma(persist_directory='dbdir', embedding_function=OpenAIEmbeddings())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "Could not load Llama model from path: ./ggml-model-q4_0.bin",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/language/lib/python3.10/site-packages/langchain/llms/llamacpp.py:99\u001b[0m, in \u001b[0;36mLlamaCpp.validate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mllama_cpp\u001b[39;00m \u001b[39mimport\u001b[39;00m Llama\n\u001b[0;32m---> 99\u001b[0m     values[\u001b[39m\"\u001b[39m\u001b[39mclient\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m Llama(\n\u001b[1;32m    100\u001b[0m         model_path\u001b[39m=\u001b[39;49mmodel_path,\n\u001b[1;32m    101\u001b[0m         n_ctx\u001b[39m=\u001b[39;49mn_ctx,\n\u001b[1;32m    102\u001b[0m         n_parts\u001b[39m=\u001b[39;49mn_parts,\n\u001b[1;32m    103\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m    104\u001b[0m         f16_kv\u001b[39m=\u001b[39;49mf16_kv,\n\u001b[1;32m    105\u001b[0m         logits_all\u001b[39m=\u001b[39;49mlogits_all,\n\u001b[1;32m    106\u001b[0m         vocab_only\u001b[39m=\u001b[39;49mvocab_only,\n\u001b[1;32m    107\u001b[0m         use_mlock\u001b[39m=\u001b[39;49muse_mlock,\n\u001b[1;32m    108\u001b[0m         n_threads\u001b[39m=\u001b[39;49mn_threads,\n\u001b[1;32m    109\u001b[0m     )\n\u001b[1;32m    110\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/language/lib/python3.10/site-packages/llama_cpp/llama.py:80\u001b[0m, in \u001b[0;36mLlama.__init__\u001b[0;34m(self, model_path, n_ctx, n_parts, seed, f16_kv, logits_all, vocab_only, use_mlock, embedding, n_threads, n_batch, last_n_tokens_size, verbose)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(model_path):\n\u001b[0;32m---> 80\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel path does not exist: \u001b[39m\u001b[39m{\u001b[39;00mmodel_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx \u001b[39m=\u001b[39m llama_cpp\u001b[39m.\u001b[39mllama_init_from_file(\n\u001b[1;32m     83\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_path\u001b[39m.\u001b[39mencode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams\n\u001b[1;32m     84\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Model path does not exist: ./ggml-model-q4_0.bin",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mllms\u001b[39;00m \u001b[39mimport\u001b[39;00m LlamaCpp\n\u001b[0;32m----> 3\u001b[0m llm \u001b[39m=\u001b[39m LlamaCpp(model_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./ggml-model-q4_0.bin\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/language/lib/python3.10/site-packages/pydantic/main.py:339\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/language/lib/python3.10/site-packages/pydantic/main.py:1102\u001b[0m, in \u001b[0;36mpydantic.main.validate_model\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/language/lib/python3.10/site-packages/langchain/llms/llamacpp.py:117\u001b[0m, in \u001b[0;36mLlamaCpp.validate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mModuleNotFoundError\u001b[39;00m(\n\u001b[1;32m    112\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCould not import llama-cpp-python library. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    113\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease install the llama-cpp-python library to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39muse this embedding model: pip install llama-cpp-python\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m     )\n\u001b[1;32m    116\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNameError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not load Llama model from path: \u001b[39m\u001b[39m{\u001b[39;00mmodel_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m values\n",
      "\u001b[0;31mNameError\u001b[0m: Could not load Llama model from path: ./ggml-model-q4_0.bin"
     ]
    }
   ],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "llm = LlamaCpp(model_path=\"./ggml-model-q4_0.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'search_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m search_index\n",
      "\u001b[0;31mNameError\u001b[0m: name 'search_index' is not defined"
     ]
    }
   ],
   "source": [
    "search_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: dbdir2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'TODO.txt'}\n",
      "- update getChunks. From a number of temp .txt fil\n",
      "501\n",
      "{'source': 'article.txt'}\n",
      "Humanity Is Facing a Great Injustice. The World Ba\n",
      "507\n",
      "507\n",
      "508\n",
      "510\n",
      "508\n",
      "507\n",
      "508\n",
      "511\n",
      "508\n",
      "509\n",
      "65\n",
      "{'source': 'article4.txt'}\n",
      "Le mod`ele lin ÃÅeaire est souvent le premier outil\n",
      "511\n",
      "503\n",
      "502\n",
      "90\n",
      "{'source': 'article2.txt'}\n",
      "The Income Gap Is Becoming a Physical-Activity Div\n",
      "508\n",
      "510\n",
      "510\n",
      "510\n",
      "507\n",
      "509\n",
      "507\n",
      "511\n",
      "510\n",
      "288\n",
      "{'source': 'article3.txt'}\n",
      "Israel Is Courting Disaster\n",
      "March 5, 2023\n",
      "By Micha\n",
      "512\n",
      "510\n",
      "508\n",
      "508\n",
      "499\n",
      "512\n",
      "512\n",
      "511\n",
      "510\n",
      "504\n",
      "510\n",
      "507\n",
      "222\n"
     ]
    }
   ],
   "source": [
    "def getDocs():\n",
    "    for file in os.listdir():\n",
    "        if file.endswith(\".txt\"):\n",
    "            with open(file, \"r\") as f:\n",
    "                github_url = f\"{file}\"\n",
    "                yield Document(page_content=f.read(), metadata={\"source\": github_url})\n",
    "\n",
    "sources = getDocs()\n",
    "source_chunks = []\n",
    "splitter = CharacterTextSplitter(separator=\" \", chunk_size=512, chunk_overlap=0)\n",
    "for source in sources:\n",
    "    print(source.metadata)\n",
    "    print(source.page_content[:50])\n",
    "    for chunk in splitter.split_text(source.page_content):\n",
    "        print(len(chunk))\n",
    "        source_chunks.append(Document(page_content=chunk, metadata=source.metadata))\n",
    "        \n",
    "vectorstore = Chroma.from_documents(getDocs(), OpenAIEmbeddings(), persist_directory = 'dbdir2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4cc8a692-d529-11ed-969b-22414b0296f5']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.add_documents([Document(page_content=\"Ankush went to Princeton a fourht time!\", metadata={'source': 'sentence4'})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No embedding_function provided, using default embedding function: SentenceTransformerEmbeddingFunction\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = vectorstore._client.list_collections()[0]\n",
    "vectorstore._client._count('langchain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "language",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
